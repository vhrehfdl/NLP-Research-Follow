## 논문
- 제목 : Understanding the Difficulty of Training Transformers
- 원본 : [https://arxiv.org/abs/2004.08249](https://arxiv.org/abs/2004.08249)
- 요약 : [PDF]()
<br>


## 참여자
이정훈, 김소연, 남궁민상, 구선민, 김지성
<br><br>


## 토론
- Q1. Model stability가 무엇을 의미하는 건가요? 얼핏 봤을 때는 robustness 관련 이슈인 것 같은데... unstable model에서는 어떤 이슈가 생기게 되나요?

  >- A1. 학습시킬 때 불안정하게 학습되느냐(gradient explosion, vanishing) 아니냐의 문제로 이해했습니다. 
  >학습이 불안정하게 되면 lr, batch size 외에도 하이퍼파라미터 맞추는게 좀 까다로워지게 되죠.

<br>

- Q2. M제가 이번 논문을 읽고 가장 크게 느낀것은 normalization이 내 생각보다 중요할 수도 있겠다 였습니다. 
- 과거에 제 경험적인 생각으로 normalization은 중요하지만 성능에 지대한 영향을 끼치지는 않는다였습니다. 지금 생각해보니 그 때 다루었던 데이터가 너무 작았고 variation자체가 크지 않았기 때문에 그렇지는 않았을까 싶습니다. (만약 제가 회사에서 더 큰 데이터를 사용한다면 먼저 variation을 체크하고 이 부분을 신경쓰지 않을까 싶네요 ㅎㅎ)
- 논문에는 대용량의 데이터를 사용해 학습해서 그런건지는 몰라도 normalization의 기법 변경에 신경을 많이 쓴 논문이라 생각됩니다. 혹시 다른 분들은 이번 논문에서 어떤 부분이 가장 와닿았나요? 

  >- A2. NLP 처리하면서 normalization이 빠지고 안빠지고의 성능이나 훈련과정이 어땠는지 궁금합니다! 이 논문뿐 아니라 layer normalization의 위치에 대한 분석한 논문이 몇개 더 있는 것 같았는데 어쨌던 Transformer의 성능에서 layer noramlization이 영향이 크긴하구나.그리고 어떤 식으로 본인들이 주장하는 바를 보여주는 plot이 직관적이라는 점은 와닿았는데, model initialization을 위해 w를 도입하는게 어떤 효과가 있는지에 대해 이해를 잘못해서인지 제안하는 방법은 좀 투박하다고 느낀 것 같습니다.

  >- A2. 정훈님 말씀에 저도 공감합니다. 저도 이번 논문 읽으면서 normalization 이 생각 이상으로 성능에 영향을 주겠다 생각 들었습니다. 실제로 데이터를 다룰때 거의 형식적으로 해준다(?) 라는 느낌을 가지고 있었는데.. 저가 사용한 데이터는 모델이 받아드리기엔 조그마한 데이터였나 싶네요 ; 여러 성격이 섞여있는 데이터에서는 확실이 중요하게 작용될거 같습니다.<br>

<br>
