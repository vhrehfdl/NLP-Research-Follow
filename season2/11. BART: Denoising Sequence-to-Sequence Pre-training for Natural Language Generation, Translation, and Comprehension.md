## 논문
- 제목 : BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
- 원본 : [https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461)
- 요약 : [PDF](https://github.com/vhrehfdl/NLP-Research-Follow/blob/main/season2/summary/11.BART-Denoising_Sequence-to-Sequence_Pre-training_for_Natural_Language_Genration_Translation_and_Comprehension.pdf)
<br>


## 참여자
이정훈, 남궁민상, 김지성, 구선민
<br><br>


## 토론
- Q1. 참 단순하면서 어려운 질문인데 BART가 성능이 좋은 이유가 무엇이라고 생각하시나요? 논문에서 말한 denosing autoencoder 방식 때문인지, 모델의 구조가 변경되었기 때문인지, 학습 데이터가 늘어났기 때문인지..? 해당 논문들을 읽고 어떻게 받아들였는지 여러분들의 생각이 궁금합니다.

  >- A1. 제가 학습데이터를 늘렸는 부분은 확인을 못해서요. 개인적으로는 1) 논문에서 autoencoder 학습시킬 때 pre-training BART 기법을 비중있게 다루기도 하고, 2) Table1 에서 공통된 모델 BART base로 성능 향상을 분석한걸 보면 denosing autoencoder가 영향력이 가장 클 것 같긴합니다. 다만,  2.1 섹션에서 모델 구조 설명( self attention은 좋으니까 갖다붙인거같고 fc는 self attention붙였으니까 뺀 것 같은데 ) 에 따라 위의 denosing 부분이 attention을 붙임으로 인해 더 유의미하게 작동했는진 모르겠네요. 결국 두개를 같이 씀으로써 상승 효과가 잇었을수도..?

  >- A1. 개인적인 생각은 task by task 이지만 DAE (denosing autoencoder) 방식을 사용한 부분이 성능향상에 기여한 주된 이유가 아닐까 생각합니다. 특히 BART는 generation task, summarization task에서 큰 성능향을 보였는데,  1) 다양한 Denoising object 를 적용할수 있고, 2) Text infilling 같이 span length 마저도 학습으로 추론하게 하는 방식 등이 유효하게 작용했지 않았을까 .. 하는 생각입니다.

<br>

- Q2. BART가 강점을 발휘하는 Task는 어디일까요? 그 이외에도 BERT, XLNet, RoBERTa와 같이 각각의 pre trained model이 특별히 강점을 발휘하는 Task가 있을까요?

  >- A2. 여러 autoencoder 계열 논문이 이미지 generation 테스크에서 많이 적용되는만큼 BART도 generation task에 더 유용하게 쓰이는 것 같아보이네요. 다만.. 제가 이번에 질문받은 것 중에  autoencoder 계열에서 bottleneck 구조의 dimension이 최종 generation에 어떤 효과를 줄 것이라 생각하는지 ( 더욱이 LSTM의 context vector의 dimension 사이즈는 어떤 효과가 있을지 ) 가 있었습니다. 어떻게 생각하시나요..? ( 제가 모레도 시험이 있어서 참석이 내일 참석이 어렵겠습니다 ㅠㅠ 다음주에 제 발표 준비 잘해겠습니다 ㅠㅠ 그리고 질문에 대한 생각들 들려주셔요~ )

  >- A2. 논문에서는 BART의 강점을 찝어서 이야기하진 않지만 generation task, 특히 text summarization task 라고 생각합니다.

<br>
